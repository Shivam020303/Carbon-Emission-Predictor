{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe22960",
   "metadata": {},
   "source": [
    "DATA ANALYSIS:\n",
    "\n",
    "1. The Given data provide various details (series) like CO2 emissions per capita (metric tons), Energy use per capita (kilograms of oil equivalent), Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $) etc. of differrents countries and each series has it's own series code.\n",
    "\n",
    "2. The other columns are scale, decimals, country name, and data of years 1990 to 2011.\n",
    "\n",
    "3. We could also notice some values missing or marked \"..\" or NaN which need to be cleaned appropiately use python coding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b294227",
   "metadata": {},
   "source": [
    "ANALYSING DATA USING PYTHON:\n",
    "\n",
    "1. Import necessary modules.\n",
    "2. Load in your excel datasheet.\n",
    "3. Check no. of rows and columns.\n",
    "4. Get the names of columns.\n",
    "5. Get data type of each column and check if it's according to our requirement.\n",
    "6. Overview of the data by checking the first ten rows and ensuring suitable data for them, so that they can be further optimized.\n",
    "7. Descriptive analysis of all the columns.\n",
    "8. Using Unique() function in each column to check of anamoly/ies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4160b",
   "metadata": {},
   "source": [
    "STEP 1 : Import liberaries and loading up the excel datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81f4027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#LOADING DATA FROM EXCEL FILE\n",
    "# define the file name and the data sheet\n",
    "orig_data_file = r\"Original Data.xls\"\n",
    "data_sheet = \"Data\"\n",
    "\n",
    "# read the data from the excel file to a pandas DataFrame\n",
    "data_orig = pd.read_excel(io=orig_data_file, sheet_name=data_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6616f",
   "metadata": {},
   "source": [
    "STEP 2 : GLOBAL DATA OVERVIEW\n",
    "\n",
    "-> Shape of excel sheet.\n",
    "\n",
    "-> Availalble columns and their datatypes.\n",
    "\n",
    "-> Overview of first 10 rows.\n",
    "\n",
    "-> Descriptive statistics of the excel sheet.\n",
    "\n",
    "-> Using Unique() to check for irregularities in different columns and thus aknowledging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original dataset: (13512, 28)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Available columns: Index(['Country code', 'Country name',  'Series code',  'Series name',\n",
      "              'SCALE',     'Decimals',           1990,           1991,\n",
      "                 1992,           1993,           1994,           1995,\n",
      "                 1996,           1997,           1998,           1999,\n",
      "                 2000,           2001,           2002,           2003,\n",
      "                 2004,           2005,           2006,           2007,\n",
      "                 2008,           2009,           2010,           2011],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Column data types: Country code    object\n",
      "Country name    object\n",
      "Series code     object\n",
      "Series name     object\n",
      "SCALE           object\n",
      "Decimals        object\n",
      "1990            object\n",
      "1991            object\n",
      "1992            object\n",
      "1993            object\n",
      "1994            object\n",
      "1995            object\n",
      "1996            object\n",
      "1997            object\n",
      "1998            object\n",
      "1999            object\n",
      "2000            object\n",
      "2001            object\n",
      "2002            object\n",
      "2003            object\n",
      "2004            object\n",
      "2005            object\n",
      "2006            object\n",
      "2007            object\n",
      "2008            object\n",
      "2009            object\n",
      "2010            object\n",
      "2011            object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of the first 5 rows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Descriptive statistics of the columns:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-> Shape of excel sheet.\n",
    "print(\"Shape of the original dataset:\",data_orig.shape)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-> Availalble columns and their datatypes.\n",
    "print(\"Available columns:\",data_orig.columns)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Column data types:\",data_orig.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-> Overview of first 10 rows.\n",
    "print(\"Overview of the first 5 rows:\")\n",
    "data_orig.head()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-> Descriptive statistics of the excel sheet.\n",
    "print(\"Descriptive statistics of the columns:\")\n",
    "data_orig.describe()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-> Using Unique() to check for irregularities in different columns and thus aknowledging them.\n",
    "data_orig['Series name'].unique()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "data_orig['Series code'].unique()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "data_orig['SCALE'].unique()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "data_orig['Decimals'].unique()\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-> Aknowledging irregularities in the columns.\n",
    "data_orig[data_orig['SCALE']=='Text']\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "data_orig[data_orig['Decimals']=='Text']\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459f0a9",
   "metadata": {},
   "source": [
    "STEP 3 : FINDING FROMM THE GLOBAL OVERVIEW:\n",
    "\n",
    "-> This global overview gives away the following facts about the available data:\n",
    "* shape: 28 columns, 13512 rows\n",
    "* all columns are of type \"object\" - neither numeric, nor string/text values\n",
    "* A certain amount of missing values, denoted both as NaN (not a number values) and as the string \"..\"\n",
    "* The rows marked as *'Text'* in the columns *'SCALE'* and *'Decimals'* do not contain any information, almost completely composed of NaN values\n",
    "* The columns represent key values such as country, but also the corresponding years and the series code/name\n",
    "* The columns *'Country name'*, *'Series code'*, *'SCALE'* and *'Decimals'* do not give any information and are therefore obsolete\n",
    "* The column *'Series name'* contains the country-specific features required for the analysis\n",
    "* The names of the features in the column *'Series name'* are clear but too long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a4054",
   "metadata": {},
   "source": [
    "DATA CLEANING: \n",
    "\n",
    "The objective is to restructure the dataset by turning the 'Series name' values into distinct features, ensuring each row corresponds to a unique combination of country and year. To enhance clarity, year values will be consolidated into a single column.\n",
    "\n",
    "To ensure data integrity and consistency, the following preprocessing steps will be taken:\n",
    "- Eliminate any rows where the \"SCALE\" or \"Decimals\" columns contain the text \"Text\".\n",
    "- Remove extraneous columns: \"Country name\", \"Series code\", \"SCALE\", and \"Decimals\".\n",
    "- Replace all instances of \"..\" and blank cells (\"\") with NaN to standardize missing data.\n",
    "- Convert data columns to numerical types for accurate processing.\n",
    "- Clean and rename entries in the \"Series name\" column to create valid and interpretable feature names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cb0c8",
   "metadata": {},
   "source": [
    "STEPS IN DATA CLEANING:\n",
    "\n",
    "1.  Create a new Data Frame object and alter it by removing entries with \"SCALE\" and \"DECIMAL\" values assigned as \"TEXT\" are deleted. Thus ensure this by checking the no. of rows and columns in original and new file.\n",
    "2. Drop 'Country name', 'Series code', 'SCALE' and  'Decimals' columns as they are not required for our program.\n",
    "3. Replace \"\" and \"..\" object values with NaN to avoid misinterpretetion by python.\n",
    "4. Convert columns 1990 to 2011 to numeric data type.\n",
    "5. Rename the features in column \"Series name\":\n",
    "\n",
    "   The variable/feature names in the column *'Series name'* are clear, but too long and not practical to use in the code. In order to improve that, the most relevant feature names will be renamed with shorter labels as indicated in the following table:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <b>Variable name</b> </td>\n",
    "        <td> <b>Description</b> </td>\n",
    "        <td> <b>Unit</b> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> cereal_yield </td>\n",
    "        <td> Cereal yield </td>\n",
    "        <td> kg per hectare </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> fdi_perc_gdp </td>\n",
    "        <td> Foreign direct investment, net inflows </td>\n",
    "        <td> % of GDP </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> elec_access_perc </td>\n",
    "        <td> Access to electricity </td>\n",
    "        <td> % of total population </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> en_per_gdp </td>\n",
    "        <td> Energy use per units of GDP </td>\n",
    "        <td> kg oil eq./\\$1,000 of 2005 PPP \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> en_per_cap </td>\n",
    "        <td> Energy use per capita </td>\n",
    "        <td> kilograms of oil equivalent </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_ttl </td>\n",
    "        <td> CO2 emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_per_cap </td>\n",
    "        <td> CO2 emissions, total </td>\n",
    "        <td> metric tons </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_per_gdp </td>\n",
    "        <td> CO2 emissions per units of GDP </td>\n",
    "        <td> kg/\\$1,000 of 2005 PPP \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> other_ghg_ttl </td>\n",
    "        <td> Other GHG emissions, total </td>\n",
    "        <td> KtCO2e </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> ch4_ttl </td>\n",
    "        <td> Methane (CH4) emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> n2o_ttl </td>\n",
    "        <td> Nitrous oxide (N2O) emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> nat_emerg </td>\n",
    "        <td> Droughts, floods, extreme temps </td>\n",
    "        <td> % pop. avg. 1990-2009 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop_urb_aggl_perc </td>\n",
    "        <td> Population in urban agglomerations >1million </td>\n",
    "        <td> % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> prot_area_perc </td>\n",
    "        <td> Nationally terrestrial protected areas </td>\n",
    "        <td> % of total land area </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> gdp </td>\n",
    "        <td> Gross Domestic Product (GDP) </td>\n",
    "        <td> \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> gni_per_cap </td>\n",
    "        <td> GNI per capita </td>\n",
    "        <td> Atlas \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> under_5_mort_rate </td>\n",
    "        <td> Under-five mortality rate </td>\n",
    "        <td> per 1,000 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop_growth_perc </td>\n",
    "        <td> Population growth </td>\n",
    "        <td> annual % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop </td>\n",
    "        <td> Population </td>\n",
    "        <td> 1 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> urb_pop_growth_perc </td>\n",
    "        <td> Urban population growth </td>\n",
    "        <td> annual % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> urb_pop </td>\n",
    "        <td> Urban population </td>\n",
    "        <td> 1 </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 13512\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unnecessary rows removed from the SCALE and DECIMALS column.\n",
      "Current number of rows:\n",
      "10017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Original number of columns:\n",
      "28\n",
      "Current number of columns:\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25668\\448712904.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_clean.iloc[:,2:] = data_clean.iloc[:,2:].replace({'':np.nan, '..':np.nan})\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25668\\448712904.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25668\\448712904.py:32: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the column data types after transformation: Country code     object\n",
      "Series name      object\n",
      "1990            float64\n",
      "1991            float64\n",
      "1992            float64\n",
      "1993            float64\n",
      "1994            float64\n",
      "1995            float64\n",
      "1996            float64\n",
      "1997            float64\n",
      "1998            float64\n",
      "1999            float64\n",
      "2000            float64\n",
      "2001            float64\n",
      "2002            float64\n",
      "2003            float64\n",
      "2004            float64\n",
      "2005            float64\n",
      "2006            float64\n",
      "2007            float64\n",
      "2008            float64\n",
      "2009            float64\n",
      "2010            float64\n",
      "2011            float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign the data to a new DataFrame, which will be modified\n",
    "data_clean = data_orig\n",
    "\n",
    "print(\"Original number of rows:\",data_clean.shape[0])\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# remove rows characterized as \"Text\" in the SCALE column\n",
    "data_clean = data_clean[data_clean['SCALE']!='Text']\n",
    "data_clean = data_clean[data_clean['Decimals']!='Text']\n",
    "\n",
    "print(\"Unnecessary rows removed from the SCALE and DECIMALS column.\")\n",
    "print(\"Current number of rows:\")\n",
    "print(data_clean.shape[0])\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# remove the columns that are not needed for further analysis\n",
    "# 'Country name', 'Series code', 'SCALE', and 'Decimals' are not needed for the analysis\n",
    "print(\"Original number of columns:\")\n",
    "print(data_clean.shape[1])\n",
    "\n",
    "data_clean = data_clean.drop(['Country name', 'Series code', 'SCALE', 'Decimals'], axis='columns')\n",
    "\n",
    "print(\"Current number of columns:\")\n",
    "print(data_clean.shape[1])\n",
    "\n",
    "# replace empty strings and '..' with NaN in the data columns\n",
    "data_clean.iloc[:,2:] = data_clean.iloc[:,2:].replace({'':np.nan, '..':np.nan})\n",
    "\n",
    "# convert the data columns to numeric, forcing errors to NaN\n",
    "data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "# Errors are ignored in order to avoid error messages about the first two columns, which don't need to be transformed\n",
    "# into numeric type anyway\n",
    "\n",
    "print(\"Print the column data types after transformation:\",\"\\n\",data_clean2.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# define shorter names corresponding to most relevant variables in a dictionary\n",
    "chosen_vars = {'Cereal yield (kg per hectare)': 'cereal_yield',\n",
    "               'Foreign direct investment, net inflows (% of GDP)': 'fdi_perc_gdp',\n",
    "               'Access to electricity (% of total population)': 'elec_access_perc',\n",
    "               'Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $)': 'en_per_gdp',\n",
    "               'Energy use per capita (kilograms of oil equivalent)': 'en_per_cap',\n",
    "               'CO2 emissions, total (KtCO2)': 'co2_ttl',\n",
    "               'CO2 emissions per capita (metric tons)': 'co2_per_cap',\n",
    "               'CO2 emissions per units of GDP (kg/$1,000 of 2005 PPP $)': 'co2_per_gdp',\n",
    "               'Other GHG emissions, total (KtCO2e)': 'other_ghg_ttl',\n",
    "               'Methane (CH4) emissions, total (KtCO2e)': 'ch4_ttl',\n",
    "               'Nitrous oxide (N2O) emissions, total (KtCO2e)': 'n2o_ttl',\n",
    "               'Droughts, floods, extreme temps (% pop. avg. 1990-2009)': 'nat_emerg',\n",
    "               'Population in urban agglomerations >1million (%)': 'pop_urb_aggl_perc',\n",
    "               'Nationally terrestrial protected areas (% of total land area)': 'prot_area_perc',\n",
    "               'GDP ($)': 'gdp',\n",
    "               'GNI per capita (Atlas $)': 'gni_per_cap',\n",
    "               'Under-five mortality rate (per 1,000)': 'under_5_mort_rate',\n",
    "               'Population growth (annual %)': 'pop_growth_perc',\n",
    "               'Population': 'pop',\n",
    "               'Urban population growth (annual %)': 'urb_pop_growth_perc',\n",
    "               'Urban population': 'urb_pop'\n",
    "                }\n",
    "\n",
    "# rename all variables in the column \"Series name\" with comprehensible shorter versions\n",
    "data_clean2['Series name'] = data_clean2['Series name'].replace(to_replace=chosen_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703ab35",
   "metadata": {},
   "source": [
    "DATA CLEANING : STAGE 2 : \n",
    "\n",
    "* Analysing the datasheet we observe we can alter it's orientation to improve readablity and ease of apply coding techniques to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c8df9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country code</th>\n",
       "      <th>Country name</th>\n",
       "      <th>Series code</th>\n",
       "      <th>Series name</th>\n",
       "      <th>SCALE</th>\n",
       "      <th>Decimals</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>...</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>AG.LND.EL5M.ZS</td>\n",
       "      <td>Land area below 5m (% of land area)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.574810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADO</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AG.LND.EL5M.ZS</td>\n",
       "      <td>Land area below 5m (% of land area)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AG.LND.EL5M.ZS</td>\n",
       "      <td>Land area below 5m (% of land area)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>AG.LND.EL5M.ZS</td>\n",
       "      <td>Land area below 5m (% of land area)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>AG.LND.EL5M.ZS</td>\n",
       "      <td>Land area below 5m (% of land area)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.967875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country code Country name     Series code  \\\n",
       "0          ABW        Aruba  AG.LND.EL5M.ZS   \n",
       "1          ADO      Andorra  AG.LND.EL5M.ZS   \n",
       "2          AFG  Afghanistan  AG.LND.EL5M.ZS   \n",
       "3          AGO       Angola  AG.LND.EL5M.ZS   \n",
       "4          ALB      Albania  AG.LND.EL5M.ZS   \n",
       "\n",
       "                           Series name  SCALE  Decimals       1990  1991  \\\n",
       "0  Land area below 5m (% of land area)      0         1  29.574810   NaN   \n",
       "1  Land area below 5m (% of land area)      0         1   0.000000   NaN   \n",
       "2  Land area below 5m (% of land area)      0         1   0.000000   NaN   \n",
       "3  Land area below 5m (% of land area)      0         1   0.208235   NaN   \n",
       "4  Land area below 5m (% of land area)      0         1   4.967875   NaN   \n",
       "\n",
       "   1992  1993  ...  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  \n",
       "0   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the short feature names into a list of strings\n",
    "chosen_cols = list(chosen_vars.values())\n",
    "\n",
    "# define an empty list, where sub-dataframes for each feature will be saved\n",
    "frame_list = []\n",
    "\n",
    "# iterate over all chosen features\n",
    "for variable in chosen_cols:\n",
    "\n",
    "    # pick only rows corresponding to the current feature\n",
    "    frame = data_clean2[data_clean2['Series name'] == variable]\n",
    "\n",
    "    # melt all the values for all years into one column and rename the columns correspondingly\n",
    "    frame = frame.melt(id_vars=['Country code', 'Series name']).rename(columns={'Country code': 'country', 'variable': 'year', 'value': variable}).drop(['Series name'], axis='columns')\n",
    "\n",
    "    # add the melted dataframe for the current feature into the list\n",
    "    frame_list.append(frame)\n",
    "\n",
    "\n",
    "# merge all sub-frames into a single dataframe, making an outer binding on the key columns 'country','year'\n",
    "from functools import reduce\n",
    "all_vars = reduce(lambda left, right: pd.merge(left, right, on=['country','year'], how='outer'), frame_list)\n",
    "\n",
    "\n",
    "data_clean2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88435b54",
   "metadata": {},
   "source": [
    "DATA CLEANING : STAGE 3 : \n",
    "\n",
    "* Handling Missing values but also trying to preserve the most of the data.\n",
    "\n",
    "-> Step 1: find total no. of missing values in each columns and sort them is ascending order.\n",
    "\n",
    "-> Step 2: Find total no. of missing values for each year rows.\n",
    "\n",
    "-> Step 3: Analyze the outcomes.\n",
    "\n",
    "-> Step 4: Optimize the data accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d3696d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the amount of missing values in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "country                   0\n",
       "year                      0\n",
       "cereal_yield           1377\n",
       "fdi_perc_gdp           1111\n",
       "elec_access_perc       5027\n",
       "en_per_gdp             2082\n",
       "en_per_cap             1956\n",
       "co2_ttl                1143\n",
       "co2_per_cap            1146\n",
       "co2_per_gdp            1557\n",
       "other_ghg_ttl          4542\n",
       "ch4_ttl                4526\n",
       "n2o_ttl                4526\n",
       "nat_emerg              4958\n",
       "pop_urb_aggl_perc      2582\n",
       "prot_area_perc          726\n",
       "gdp                     779\n",
       "gni_per_cap            1013\n",
       "under_5_mort_rate       716\n",
       "pop_growth_perc         282\n",
       "pop                     252\n",
       "urb_pop_growth_perc     494\n",
       "urb_pop                 467\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"check the amount of missing values in each column\")\n",
    "all_vars.isnull().sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5bc8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values by year:\n",
      "Country name : 2\n",
      "Decimals : 2\n",
      "SCALE : 2\n",
      "Series code : 2\n",
      "2005 : 1189\n",
      "2000 : 1273\n",
      "1995 : 1317\n",
      "1990 : 1427\n",
      "2007 : 1631\n",
      "2006 : 1633\n",
      "2004 : 1646\n",
      "2008 : 1708\n",
      "2003 : 1714\n",
      "2002 : 1715\n",
      "2001 : 1718\n",
      "1999 : 1729\n",
      "1998 : 1739\n",
      "1997 : 1746\n",
      "1996 : 1756\n",
      "1994 : 1781\n",
      "1993 : 1792\n",
      "1992 : 1810\n",
      "1991 : 1921\n",
      "2009 : 2078\n",
      "2010 : 3038\n",
      "2011 : 4893\n"
     ]
    }
   ],
   "source": [
    "all_vars_clean = all_vars\n",
    "\n",
    "#define an array with the unique year values\n",
    "years_count_missing = dict.fromkeys(all_vars_clean['year'].unique(), 0)\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    years_count_missing[row['year']] += row.isnull().sum()\n",
    "\n",
    "# sort the years by missing values\n",
    "years_missing_sorted = dict(sorted(years_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each year\n",
    "print(\"missing values by year:\")\n",
    "for key, val in years_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a01958",
   "metadata": {},
   "source": [
    "DATA OUTCOME ANALYSIS:\n",
    "\n",
    "* The years 1991 to 2008 offer a good balance and are selected for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "162d8107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the years:\n",
      "41262\n",
      "number of rows before filtering the years:\n",
      "6058\n",
      "Column data types: \n",
      " country                object\n",
      "year                   object\n",
      "cereal_yield           object\n",
      "fdi_perc_gdp           object\n",
      "elec_access_perc       object\n",
      "en_per_gdp             object\n",
      "en_per_cap             object\n",
      "co2_ttl                object\n",
      "co2_per_cap            object\n",
      "co2_per_gdp            object\n",
      "other_ghg_ttl          object\n",
      "ch4_ttl                object\n",
      "n2o_ttl                object\n",
      "nat_emerg              object\n",
      "pop_urb_aggl_perc      object\n",
      "prot_area_perc         object\n",
      "gdp                    object\n",
      "gni_per_cap            object\n",
      "under_5_mort_rate      object\n",
      "pop_growth_perc        object\n",
      "pop                    object\n",
      "urb_pop_growth_perc    object\n",
      "urb_pop                object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25668\\2232159134.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  all_vars_clean = all_vars_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25668\\2232159134.py:7: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  all_vars_clean = all_vars_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data types: \n",
      " country                object\n",
      "year                   object\n",
      "cereal_yield           object\n",
      "fdi_perc_gdp           object\n",
      "elec_access_perc       object\n",
      "en_per_gdp             object\n",
      "en_per_cap             object\n",
      "co2_ttl                object\n",
      "co2_per_cap            object\n",
      "co2_per_gdp            object\n",
      "other_ghg_ttl          object\n",
      "ch4_ttl                object\n",
      "n2o_ttl                object\n",
      "nat_emerg              object\n",
      "pop_urb_aggl_perc      object\n",
      "prot_area_perc         object\n",
      "gdp                    object\n",
      "gni_per_cap            object\n",
      "under_5_mort_rate      object\n",
      "pop_growth_perc        object\n",
      "pop                    object\n",
      "urb_pop_growth_perc    object\n",
      "urb_pop                object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mColumn data types:\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,all_vars_clean.dtypes)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# filter only rows for years between 1991 and 2008 (having less missing values)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m all_vars_clean = all_vars_clean[(\u001b[43mall_vars_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1991\u001b[39;49m) & (all_vars_clean[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] <= \u001b[32m2008\u001b[39m)]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumber of missing values in the whole dataset after filtering the years:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_vars_clean.isnull().sum().sum())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Edunet Internship\\Week 1\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Edunet Internship\\Week 1\\.venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Edunet Internship\\Week 1\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6130\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6127\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6128\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Edunet Internship\\Week 1\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lvalues.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     res_values = \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Edunet Internship\\Week 1\\.venv\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:129\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    127\u001b[39m     result = libops.vec_compare(x.ravel(), y.ravel(), op)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     result = \u001b[43mlibops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(x.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/ops.pyx:107\u001b[39m, in \u001b[36mpandas._libs.ops.scalar_compare\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: '>=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the years:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "print(\"Column data types:\",\"\\n\",all_vars_clean.dtypes)\n",
    "all_vars_clean = all_vars_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "print(\"Column data types:\",\"\\n\",all_vars_clean.dtypes)\n",
    "\n",
    "# filter only rows for years between 1991 and 2008 (having less missing values)\n",
    "all_vars_clean = all_vars_clean[(all_vars_clean['year'] >= 1991) & (all_vars_clean['year'] <= 2008)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the years:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968ead9",
   "metadata": {},
   "source": [
    "Filtering the countries by missing values:\n",
    "\n",
    "The same procedure is applied to the filtering of countries with missing values. The following snippet shows the number of NaNs for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1144be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values by country:\n",
      "CHL : 120\n",
      "ISR : 120\n",
      "AUT : 121\n",
      "CHE : 121\n",
      "DEU : 121\n",
      "DNK : 121\n",
      "ESP : 121\n",
      "FIN : 121\n",
      "FRA : 121\n",
      "GBR : 121\n",
      "HUN : 121\n",
      "IRL : 121\n",
      "ITA : 121\n",
      "JPN : 121\n",
      "KOR : 121\n",
      "MEX : 121\n",
      "NLD : 121\n",
      "PRT : 121\n",
      "SWE : 121\n",
      "TUR : 121\n",
      "USA : 121\n",
      "AGO : 122\n",
      "ARG : 122\n",
      "BGD : 122\n",
      "BOL : 122\n",
      "BRA : 122\n",
      "CAN : 122\n",
      "CIV : 122\n",
      "CMR : 122\n",
      "COG : 122\n",
      "COL : 122\n",
      "CRI : 122\n",
      "DOM : 122\n",
      "ECU : 122\n",
      "EGY : 122\n",
      "EMU : 122\n",
      "GHA : 122\n",
      "GRC : 122\n",
      "GTM : 122\n",
      "HND : 122\n",
      "IDN : 122\n",
      "IND : 122\n",
      "JOR : 122\n",
      "KEN : 122\n",
      "MAR : 122\n",
      "MOZ : 122\n",
      "MYS : 122\n",
      "NGA : 122\n",
      "PAK : 122\n",
      "PAN : 122\n",
      "PER : 122\n",
      "PHL : 122\n",
      "PRY : 122\n",
      "SDN : 122\n",
      "SEN : 122\n",
      "SLV : 122\n",
      "SYR : 122\n",
      "TGO : 122\n",
      "THA : 122\n",
      "TZA : 122\n",
      "URY : 122\n",
      "VEN : 122\n",
      "VNM : 122\n",
      "ZAF : 122\n",
      "ZMB : 122\n",
      "BGR : 123\n",
      "CHN : 123\n",
      "LAC : 123\n",
      "LMC : 123\n",
      "LMY : 123\n",
      "MIC : 123\n",
      "POL : 123\n",
      "ROM : 123\n",
      "SAS : 123\n",
      "SAU : 123\n",
      "UMC : 123\n",
      "ZAR : 123\n",
      "MNA : 124\n",
      "NZL : 124\n",
      "AUS : 125\n",
      "DZA : 125\n",
      "WLD : 126\n",
      "ETH : 127\n",
      "LIC : 127\n",
      "SSA : 127\n",
      "YEM : 127\n",
      "ARE : 131\n",
      "BEL : 131\n",
      "ECA : 133\n",
      "IRN : 133\n",
      "UKR : 133\n",
      "LBN : 134\n",
      "RUS : 134\n",
      "ARM : 135\n",
      "BLR : 135\n",
      "CZE : 135\n",
      "UZB : 135\n",
      "KAZ : 136\n",
      "NIC : 138\n",
      "AZE : 139\n",
      "GEO : 140\n",
      "HTI : 140\n",
      "NOR : 142\n",
      "BEN : 143\n",
      "BWA : 143\n",
      "HIC : 143\n",
      "JAM : 143\n",
      "LKA : 143\n",
      "SGP : 143\n",
      "TTO : 143\n",
      "TUN : 143\n",
      "GAB : 144\n",
      "MNG : 144\n",
      "ALB : 146\n",
      "KHM : 146\n",
      "CYP : 147\n",
      "NPL : 147\n",
      "OMN : 147\n",
      "EAP : 148\n",
      "MLT : 148\n",
      "NAM : 150\n",
      "EST : 152\n",
      "LUX : 152\n",
      "LVA : 154\n",
      "SVN : 154\n",
      "LBY : 155\n",
      "SVK : 155\n",
      "HRV : 156\n",
      "MDA : 156\n",
      "TJK : 156\n",
      "KGZ : 157\n",
      "LTU : 157\n",
      "BRN : 158\n",
      "MKD : 158\n",
      "TKM : 158\n",
      "KWT : 162\n",
      "ZWE : 162\n",
      "ISL : 163\n",
      "CUB : 167\n",
      "IRQ : 168\n",
      "BHR : 169\n",
      "HKG : 172\n",
      "BFA : 174\n",
      "ERI : 174\n",
      "MDG : 174\n",
      "UGA : 174\n",
      "GIN : 175\n",
      "MLI : 175\n",
      "NER : 175\n",
      "BIH : 176\n",
      "MUS : 185\n",
      "BLZ : 186\n",
      "COM : 186\n",
      "CPV : 186\n",
      "FJI : 186\n",
      "GNB : 186\n",
      "GUY : 186\n",
      "SWZ : 186\n",
      "VCT : 186\n",
      "VUT : 186\n",
      "ATG : 187\n",
      "DMA : 187\n",
      "GRD : 187\n",
      "QAT : 187\n",
      "BHS : 188\n",
      "BRB : 188\n",
      "SLB : 188\n",
      "SUR : 190\n",
      "GMB : 191\n",
      "DJI : 192\n",
      "LAO : 195\n",
      "MWI : 195\n",
      "BDI : 196\n",
      "CAF : 196\n",
      "MRT : 196\n",
      "PNG : 196\n",
      "RWA : 196\n",
      "SLE : 196\n",
      "TCD : 196\n",
      "BTN : 199\n",
      "LBR : 202\n",
      "SYC : 206\n",
      "WSM : 206\n",
      "KNA : 207\n",
      "LCA : 207\n",
      "SRB : 207\n",
      "TON : 207\n",
      "KIR : 208\n",
      "GNQ : 209\n",
      "SID : 216\n",
      "MDV : 219\n",
      "PLW : 222\n",
      "MMR : 224\n",
      "PRK : 224\n",
      "AFG : 228\n",
      "MHL : 240\n",
      "FSM : 242\n",
      "LSO : 244\n",
      "STP : 256\n",
      "TMP : 259\n",
      "MAC : 261\n",
      "SOM : 262\n",
      "NCL : 267\n",
      "ADO : 271\n",
      "WBG : 277\n",
      "GRL : 280\n",
      "BMU : 290\n",
      "MNE : 290\n",
      "PRI : 292\n",
      "ABW : 294\n",
      "LIE : 297\n",
      "MCO : 297\n",
      "PYF : 298\n",
      "CYM : 318\n",
      "FRO : 328\n",
      "GIB : 330\n",
      "GUM : 338\n",
      "TUV : 338\n",
      "SMR : 339\n",
      "COK : 341\n",
      "NIU : 341\n",
      "IMY : 354\n",
      "VIR : 354\n",
      "ASM : 357\n",
      "MNP : 358\n",
      "CHI : 360\n",
      "NRU : 361\n",
      "TCA : 372\n",
      "KSV : 397\n",
      "MYT : 400\n",
      "MAF : 420\n",
      "CUW : 437\n",
      "SXM : 437\n"
     ]
    }
   ],
   "source": [
    "# check the amount of missing values by country\n",
    "\n",
    "# define an array with the unique country values\n",
    "countries_count_missing = dict.fromkeys(all_vars_clean['country'].unique(), 0)\n",
    "\n",
    "# iterate through all rows and count the amount of NaN values for each country\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    countries_count_missing[row['country']] += row.isnull().sum()\n",
    "\n",
    "# sort the countries by missing values\n",
    "countries_missing_sorted = dict(sorted(countries_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each country\n",
    "print(\"missing values by country:\")\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a371c7",
   "metadata": {},
   "source": [
    "This output would suggest to remove rows for countries with more than 90 missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be73316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the countries:\n",
      "41262\n",
      "number of rows before filtering the countries:\n",
      "6058\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'countries_missing_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# filter only rows for countries with less than 90 missing values\u001b[39;00m\n\u001b[32m      8\u001b[39m countries_filter = []\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcountries_missing_sorted\u001b[49m.items():\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val<\u001b[32m90\u001b[39m:\n\u001b[32m     11\u001b[39m         countries_filter.append(key)\n",
      "\u001b[31mNameError\u001b[39m: name 'countries_missing_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "\n",
    "# filter only rows for countries with less than 90 missing values\n",
    "countries_filter = []\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    if val<90:\n",
    "        countries_filter.append(key)\n",
    "\n",
    "all_vars_clean = all_vars_clean[all_vars_clean['country'].isin(countries_filter)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bed5a",
   "metadata": {},
   "source": [
    "DATA CLEANING : STAGE 3 : \n",
    " \n",
    "\n",
    "* Identifying columns with more no. of NaN values and thus dropping them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92058545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values per column:\n",
      "country    0\n",
      "year       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# No. of NaN values in each column before filtering\n",
    "all_vars_clean.isnull().sum()\n",
    "\n",
    "# remove features with more than 20 missing values\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "# create a boolean mapping of features with more than 20 missing values\n",
    "vars_bad = all_vars_clean.isnull().sum()>20\n",
    "\n",
    "# remove the columns corresponding to the mapping of the features with many missing values\n",
    "all_vars_clean2 = all_vars_clean.drop(compress(data = all_vars_clean.columns, selectors = vars_bad), axis='columns')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean2.isnull().sum())\n",
    "\n",
    "\n",
    "# delete rows with any number of missing values\n",
    "all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean3.isnull().sum())\n",
    "\n",
    "print(\"Final shape of the cleaned dataset:\")\n",
    "print(all_vars_clean3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e9b95",
   "metadata": {},
   "source": [
    "It appears that dropping all features with any no. of missing values doesn't alter the data too much so they are all dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1e849",
   "metadata": {},
   "source": [
    "STAGE 4 : EXPORTING THE CLEANED DATASHEET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8b20a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# export the clean dataframe to a csv file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mall_vars_clean3\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33mdata_cleaned.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean3' is not defined"
     ]
    }
   ],
   "source": [
    "# export the clean dataframe to a csv file\n",
    "all_vars_clean3.to_csv('data_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
